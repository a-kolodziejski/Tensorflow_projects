{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNZBpbNrgns4cs+OB/TGtg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"l9rraAgkIuDt"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"xhqZMTfnDLkM","executionInfo":{"status":"ok","timestamp":1697091389792,"user_tz":-120,"elapsed":884,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Load data (raw imdb dataset)"],"metadata":{"id":"nZM7lRb4I1OW"}},{"cell_type":"code","source":["!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vh0K2SJbJ71u","executionInfo":{"status":"ok","timestamp":1697091416086,"user_tz":-120,"elapsed":22494,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"96340cad-dad4-45f3-a187-8467eaf7692d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-12 06:16:34--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz.1’\n","\n","aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  23.1MB/s    in 5.4s    \n","\n","2023-10-12 06:16:39 (14.9 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n","\n"]}]},{"cell_type":"code","source":["# Remove train/unsup subdirectory\n","!rm -r aclImdb/train/unsup"],"metadata":{"id":"5E584s7qFGlT","executionInfo":{"status":"ok","timestamp":1697091420377,"user_tz":-120,"elapsed":1734,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Prepare validation data (50% of test data, so alltogether we have )\n","# Imports\n","import os, shutil, random\n","\n","BASE_DIR_TEST = \"aclImdb/test\"\n","ALL_DIR_POS = os.listdir(BASE_DIR_TEST + '/pos')\n","ALL_DIR_NEG = os.listdir(BASE_DIR_TEST + '/neg')\n","\n","# print(len(ALL_DIR_POS), len(ALL_DIR_NEG))\n","\n","# Define number of validation samples\n","VAL_SAMPLES = int(0.5 * len(ALL_DIR_POS + ALL_DIR_NEG))\n","\n","# Shuffle files\n","random.shuffle(ALL_DIR_POS)\n","random.shuffle(ALL_DIR_NEG)\n","\n","# Pick appropriate number of validation files\n","val_paths_pos = ALL_DIR_POS[:VAL_SAMPLES//2] # half samples will be positive\n","val_paths_neg = ALL_DIR_NEG[:VAL_SAMPLES//2] # half of samples will be negative\n","\n","# Create directories appropriate for validation files\n","os.makedirs('aclImdb/val/pos')\n","os.makedirs('aclImdb/val/neg')\n","\n","# Move all validation files into val/pos and val/neg directories\n","for file in val_paths_pos:\n","  shutil.move(src = \"aclImdb/test/pos/\" + file, dst = \"aclImdb/val/pos/\" + file)\n","\n","for file in val_paths_neg:\n","  shutil.move(src = \"aclImdb/test/neg/\" + file, dst = \"aclImdb/val/neg/\" + file)"],"metadata":{"id":"b8K6_vZnKgGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create train, validation and test datasets using `text_dataset_from_directory`"],"metadata":{"id":"7ibTI4BA3-xP"}},{"cell_type":"code","source":["BATCH_SIZE = 32\n","\n","# Create train dataset\n","train_data = tf.keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size = BATCH_SIZE)\n","\n","# Create validation dataset\n","val_data = tf.keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size = BATCH_SIZE)\n","\n","# Create test dataset\n","test_data = tf.keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size = BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zWGTVz9x5SrQ","executionInfo":{"status":"ok","timestamp":1697086234549,"user_tz":-120,"elapsed":3325,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"b0cf91d0-dfc6-439d-97a7-b74052d04b7e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 2 classes.\n","Found 12500 files belonging to 2 classes.\n","Found 12500 files belonging to 2 classes.\n"]}]},{"cell_type":"markdown","source":["# First attempt: bag of words model"],"metadata":{"id":"k7Aw-r1l9PxD"}},{"cell_type":"markdown","source":["### Preprocess data with TextVectorization layer"],"metadata":{"id":"k10ApVRo9xGJ"}},{"cell_type":"code","source":["# Only consider single words (ngram = 1) and multi-hot encode each sentence\n","tv = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                       ngrams = 1,\n","                                       output_mode = 'multi_hot')\n","\n","# Extract all messages from training data (leave targets out)\n","train_data_text_only = train_data.map(lambda x, y: x)\n","\n","# Build vocabulary from training data using TextVectorizer\n","tv.adapt(train_data_text_only)\n","\n","# Prepare processed versions of datasets (text multi-hot encoded using vocabulary trained on training data)\n","train_data_preprocessed_1_gram = train_data.map(lambda x, y: (tv(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_1_gram = val_data.map(lambda x, y: (tv(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_1_gram = test_data.map(lambda x, y: (tv(x), y), num_parallel_calls = 4)\n"],"metadata":{"id":"NsxJA3Tm93OL","executionInfo":{"status":"ok","timestamp":1697086249681,"user_tz":-120,"elapsed":6134,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Check if everything OK\n","for inputs, targets in train_data_preprocessed_1_gram:\n","  print(\"inputs.shape:\", inputs.shape)\n","  print(\"inputs.dtype:\", inputs.dtype)\n","  print(\"targets.shape:\", targets.shape)\n","  print(\"targets.dtype:\", targets.dtype)\n","  print(\"inputs[0]:\", inputs[0])\n","  print(\"targets[0]:\", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rq0wDQs7D4pc","executionInfo":{"status":"ok","timestamp":1696932797750,"user_tz":-120,"elapsed":270,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"07747cf8-1953-4afc-fd02-056ae4f8d4b5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["### Build and train simple dense classification model\n","\n","def build_model(num_tokens = 20000, hidden_units = 16):\n","  inputs = tf.keras.layers.Input(shape = (num_tokens, ))\n","  x = tf.keras.layers.Dense(hidden_units, activation = 'relu')(inputs)\n","  x = tf.keras.layers.Dropout(rate = 0.5)(x)\n","  outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n","\n","  model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n","\n","  model.compile(optimizer = tf.keras.optimizers.Adam(),\n","              loss = 'binary_crossentropy',\n","              metrics = ['accuracy'])\n","\n","  return model\n","\n","model = build_model()\n","\n","history = model.fit(train_data_preprocessed_1_gram,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_1_gram)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tV2kmf7SEPZU","executionInfo":{"status":"ok","timestamp":1696933317488,"user_tz":-120,"elapsed":13197,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"1653d012-7ca3-4851-b8c8-10126110b74c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","782/782 [==============================] - 15s 18ms/step - loss: 0.4000 - accuracy: 0.8357 - val_loss: 0.2919 - val_accuracy: 0.8882\n","Epoch 2/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.2426 - accuracy: 0.9133 - val_loss: 0.2812 - val_accuracy: 0.8854\n","Epoch 3/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.1790 - accuracy: 0.9384 - val_loss: 0.2954 - val_accuracy: 0.8822\n","Epoch 4/10\n","782/782 [==============================] - 13s 17ms/step - loss: 0.1388 - accuracy: 0.9512 - val_loss: 0.3273 - val_accuracy: 0.8817\n","Epoch 5/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.1101 - accuracy: 0.9634 - val_loss: 0.3619 - val_accuracy: 0.8805\n","Epoch 6/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.0920 - accuracy: 0.9689 - val_loss: 0.3932 - val_accuracy: 0.8782\n","Epoch 7/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.0792 - accuracy: 0.9730 - val_loss: 0.4432 - val_accuracy: 0.8722\n","Epoch 8/10\n","782/782 [==============================] - 11s 14ms/step - loss: 0.0684 - accuracy: 0.9771 - val_loss: 0.4630 - val_accuracy: 0.8738\n","Epoch 9/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.0569 - accuracy: 0.9810 - val_loss: 0.5016 - val_accuracy: 0.8764\n","Epoch 10/10\n","782/782 [==============================] - 11s 15ms/step - loss: 0.0537 - accuracy: 0.9821 - val_loss: 0.5219 - val_accuracy: 0.8721\n"]}]},{"cell_type":"markdown","source":["#### We achieve ~ 89% accuracy. Let's try the same with 2-grams and 3-grams"],"metadata":{"id":"xGOLjqC7F7Xx"}},{"cell_type":"code","source":["# Build new TextVectorizers, one for 2-grams and one for 3-grams\n","tv2 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                     output_mode = 'multi_hot',\n","                                     ngrams = 2)\n","\n","tv3 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                     output_mode = 'multi_hot',\n","                                     ngrams = 3)\n","\n","# Train both both vectorizers on train data texsts\n","tv2.adapt(train_data_text_only)\n","tv3.adapt(train_data_text_only)\n","\n","# Prepare preprocessed versions of all datasets\n","train_data_preprocessed_2_gram = train_data.map(lambda x, y: (tv2(x), y), num_parallel_calls = 4)\n","train_data_preprocessed_3_gram = train_data.map(lambda x, y: (tv3(x), y), num_parallel_calls = 4)\n","\n","val_data_preprocessed_2_gram = val_data.map(lambda x, y: (tv2(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_3_gram = val_data.map(lambda x, y: (tv3(x), y), num_parallel_calls = 4)\n","\n","test_data_preprocessed_2_gram = test_data.map(lambda x, y: (tv2(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_3_gram = test_data.map(lambda x, y: (tv3(x), y), num_parallel_calls = 4)"],"metadata":{"id":"uFwdZ-XTHRMu","executionInfo":{"status":"ok","timestamp":1696933825585,"user_tz":-120,"elapsed":72213,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Build and train models\n","model2 = build_model()\n","\n","history2 = model.fit(train_data_preprocessed_2_gram,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_2_gram)\n","\n","model3 = build_model()\n","\n","history3 = model.fit(train_data_preprocessed_3_gram,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_3_gram)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idHj0BudIZjQ","executionInfo":{"status":"ok","timestamp":1696934206834,"user_tz":-120,"elapsed":360743,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"5989c952-69d9-468f-ce26-4153883a21ca"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","782/782 [==============================] - 17s 20ms/step - loss: 0.6591 - accuracy: 0.7088 - val_loss: 0.4234 - val_accuracy: 0.8382\n","Epoch 2/10\n","782/782 [==============================] - 18s 23ms/step - loss: 0.4367 - accuracy: 0.8155 - val_loss: 0.3786 - val_accuracy: 0.8653\n","Epoch 3/10\n","782/782 [==============================] - 14s 18ms/step - loss: 0.3612 - accuracy: 0.8576 - val_loss: 0.3487 - val_accuracy: 0.8718\n","Epoch 4/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.3045 - accuracy: 0.8832 - val_loss: 0.3555 - val_accuracy: 0.8742\n","Epoch 5/10\n","782/782 [==============================] - 18s 23ms/step - loss: 0.2667 - accuracy: 0.8988 - val_loss: 0.3691 - val_accuracy: 0.8774\n","Epoch 6/10\n","782/782 [==============================] - 15s 20ms/step - loss: 0.2310 - accuracy: 0.9161 - val_loss: 0.3748 - val_accuracy: 0.8785\n","Epoch 7/10\n","782/782 [==============================] - 14s 18ms/step - loss: 0.2085 - accuracy: 0.9254 - val_loss: 0.3900 - val_accuracy: 0.8813\n","Epoch 8/10\n","782/782 [==============================] - 14s 18ms/step - loss: 0.1909 - accuracy: 0.9318 - val_loss: 0.3990 - val_accuracy: 0.8818\n","Epoch 9/10\n","782/782 [==============================] - 18s 22ms/step - loss: 0.1728 - accuracy: 0.9406 - val_loss: 0.4301 - val_accuracy: 0.8855\n","Epoch 10/10\n","782/782 [==============================] - 17s 21ms/step - loss: 0.1595 - accuracy: 0.9446 - val_loss: 0.4462 - val_accuracy: 0.8884\n","Epoch 1/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.7102 - accuracy: 0.6544 - val_loss: 0.4936 - val_accuracy: 0.7951\n","Epoch 2/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.4848 - accuracy: 0.7829 - val_loss: 0.4346 - val_accuracy: 0.8252\n","Epoch 3/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.4054 - accuracy: 0.8327 - val_loss: 0.4098 - val_accuracy: 0.8472\n","Epoch 4/10\n","782/782 [==============================] - 17s 21ms/step - loss: 0.3597 - accuracy: 0.8585 - val_loss: 0.3932 - val_accuracy: 0.8611\n","Epoch 5/10\n","782/782 [==============================] - 15s 19ms/step - loss: 0.3230 - accuracy: 0.8797 - val_loss: 0.4124 - val_accuracy: 0.8648\n","Epoch 6/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.2909 - accuracy: 0.8950 - val_loss: 0.4277 - val_accuracy: 0.8680\n","Epoch 7/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.2679 - accuracy: 0.9059 - val_loss: 0.4501 - val_accuracy: 0.8710\n","Epoch 8/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.2434 - accuracy: 0.9157 - val_loss: 0.4592 - val_accuracy: 0.8715\n","Epoch 9/10\n","782/782 [==============================] - 18s 23ms/step - loss: 0.2175 - accuracy: 0.9253 - val_loss: 0.4801 - val_accuracy: 0.8742\n","Epoch 10/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.2104 - accuracy: 0.9284 - val_loss: 0.4977 - val_accuracy: 0.8741\n"]}]},{"cell_type":"markdown","source":["#### As we can see introduction of 2-grams and 3-grams did not help. It may be due to the fact, that for all experiments the same number of 20000 tokens was used"],"metadata":{"id":"F-PYk5bxI5qp"}},{"cell_type":"markdown","source":["### Using tf-idf output mode instead of multi-hot encoding"],"metadata":{"id":"YqbhFmq9MWjt"}},{"cell_type":"code","source":["# Create another TextVectorizer\n","tv4 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                        output_mode = 'tf-idf',\n","                                        ngrams = 1)\n","\n","# Adapt TV to training dataset\n","tv4.adapt(train_data_text_only)\n","\n","# Prepare preprocessed versions of all datasets\n","train_data_preprocessed_tfidf = train_data.map(lambda x, y: (tv4(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_tfidf = val_data.map(lambda x, y: (tv4(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_tfidf = test_data.map(lambda x, y: (tv4(x), y), num_parallel_calls = 4)"],"metadata":{"id":"hHHnjq5LM59N","executionInfo":{"status":"ok","timestamp":1696935198042,"user_tz":-120,"elapsed":13221,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Check if everything OK\n","for inputs, targets in train_data_preprocessed_tfidf:\n","  print(\"inputs.shape:\", inputs.shape)\n","  print(\"inputs.dtype:\", inputs.dtype)\n","  print(\"targets.shape:\", targets.shape)\n","  print(\"targets.dtype:\", targets.dtype)\n","  print(\"inputs[0]:\", inputs[0])\n","  print(\"targets[0]:\", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXtpYmKROIaZ","executionInfo":{"status":"ok","timestamp":1696935248886,"user_tz":-120,"elapsed":259,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"3c3a8b33-d5ed-4bd0-fd79-da37b83827cb"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([19.388176   6.2761817  1.4221123 ...  0.         0.         0.       ], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["# Build and train model\n","model4 = build_model()\n","\n","history4 = model.fit(train_data_preprocessed_tfidf,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_tfidf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZ7M9PceOAgo","executionInfo":{"status":"ok","timestamp":1696935427447,"user_tz":-120,"elapsed":157187,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"54d1f163-c24f-48d0-a229-1e47721d9633"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","782/782 [==============================] - 13s 17ms/step - loss: 0.4463 - accuracy: 0.8812 - val_loss: 0.7639 - val_accuracy: 0.8045\n","Epoch 2/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.2401 - accuracy: 0.9191 - val_loss: 0.8411 - val_accuracy: 0.8278\n","Epoch 3/10\n","782/782 [==============================] - 15s 19ms/step - loss: 0.1973 - accuracy: 0.9346 - val_loss: 0.8434 - val_accuracy: 0.8375\n","Epoch 4/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1618 - accuracy: 0.9409 - val_loss: 0.8106 - val_accuracy: 0.8422\n","Epoch 5/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1479 - accuracy: 0.9451 - val_loss: 0.8962 - val_accuracy: 0.8483\n","Epoch 6/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1377 - accuracy: 0.9501 - val_loss: 0.9066 - val_accuracy: 0.8500\n","Epoch 7/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1217 - accuracy: 0.9550 - val_loss: 0.9450 - val_accuracy: 0.8521\n","Epoch 8/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1201 - accuracy: 0.9564 - val_loss: 0.8774 - val_accuracy: 0.8526\n","Epoch 9/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1083 - accuracy: 0.9586 - val_loss: 0.8971 - val_accuracy: 0.8540\n","Epoch 10/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.1041 - accuracy: 0.9607 - val_loss: 0.9094 - val_accuracy: 0.8534\n"]}]},{"cell_type":"markdown","source":["#### We obtain ~85% accuracy"],"metadata":{"id":"qUOd2gpdOVXY"}},{"cell_type":"markdown","source":["# Second attempt: sequence models"],"metadata":{"id":"-iXbUDBhPDrK"}},{"cell_type":"markdown","source":["To implement sequence model, one has to do the following steps:\n","* represent samples as integer sequences (one integer for one word),\n","* map integer to a vector to obtain vector sequences,\n","* feed these vector sequences into 1-D CNN or RNN stack of layers"],"metadata":{"id":"D5LD7DGTPNg8"}},{"cell_type":"markdown","source":["### Define TextVectorizer which for each sentence outputs sequence of integers"],"metadata":{"id":"kiyOIQMoQXKx"}},{"cell_type":"code","source":["# Truncate each review to contain only 600 characters\n","tv5 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                        output_mode = 'int',\n","                                        output_sequence_length = 600)\n","\n","# Adapting TextVectorizer\n","tv5.adapt(train_data_text_only)\n","\n","# Transforming train, validation and test datasets\n","train_data_preprocessed_int = train_data.map(lambda x, y: (tv5(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_int = val_data.map(lambda x, y: (tv5(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_int = test_data.map(lambda x, y: (tv5(x), y), num_parallel_calls = 4)\n","\n","# Check if everything OK\n","for inputs, targets in train_data_preprocessed_int:\n","  print(\"inputs.shape:\", inputs.shape)\n","  print(\"inputs.dtype:\", inputs.dtype)\n","  print(\"targets.shape:\", targets.shape)\n","  print(\"targets.dtype:\", targets.dtype)\n","  print(\"inputs[0]:\", inputs[0])\n","  print(\"targets[0]:\", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmICV_rKQhvb","executionInfo":{"status":"ok","timestamp":1697086540846,"user_tz":-120,"elapsed":4257,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"ea5ef467-5c19-4b8f-ca01-ee9a0ffa146d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 600)\n","inputs.dtype: <dtype: 'int64'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor(\n","[   29     5    56   499    93     6  1321   500    15    34  1222   141\n","     2  1283   506     5     4   774     5   709   346    36 16439     2\n","   315     5   567     2     1  1818   156    30   676    78     4  2087\n","   257    36  2908  3429  2352    37 15857 14470    25 17791    78    65\n","   240  1687    19    45     2    86   357     5     2    20   717   239\n","    39    34   216    18     2   326   357  2809     2   507    78     4\n","   467   267    64    17   254   346   889     6    94     4  2122    12\n","    35   118    77   635    65   457  1447     8    53    84   763    31\n","     2   129     5     2    18   248   409     7  2046    28   889     6\n","   837    17     2 12067     5    49    67    41     6    28     4   176\n","  2345    21     2  1818    19  9332    78     4   537    16  4103     2\n","    18   542    47   179  3526  9368     7    53    74   521     3  1250\n","   319    78     4  5321     5   862    12  2286   123  2895     2  1344\n","     7   312    15    74  4207  3429  3674     3  4651    24    32   312\n","     8   767    87     4   687  1508    69  2390     8   267   763   709\n","   267   457    61  3121     6     2   167  1336     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n","targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["### Building RNN model"],"metadata":{"id":"LGMupKgCRZUo"}},{"cell_type":"code","source":["inputs = tf.keras.layers.Input(shape = (None, ), dtype = 'int64') # one input is sequence of integers\n","# Hot-encode each integer\n","encoded = tf.one_hot(inputs, depth = 20000)\n","x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(encoded)\n","x = tf.keras.layers.Dropout(0.5)(x)\n","outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","model = tf.keras.models.Model(inputs, outputs)\n","\n","model.compile(optimizer=\"adam\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","history5 = model.fit(train_data_preprocessed_int,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_int)"],"metadata":{"id":"KrXKGh_8RqFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### The above model is very slow, since each review is encoded as 600 x 20000 matrix, which has 12000000 elements. Better encoding would be necessary, such as word embedding"],"metadata":{"id":"flcBOML1TGBy"}},{"cell_type":"markdown","source":["### Same model as above but with embedding layer"],"metadata":{"id":"IECr2dK9T5HT"}},{"cell_type":"code","source":["inputs = tf.keras.layers.Input(shape = (600, ))\n","x = tf.keras.layers.Embedding(input_dim = 20000, output_dim = 256, mask_zero = True)(inputs) # Notice mask_zero parameter\n","x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(x)\n","x = tf.keras.layers.Dropout(rate = 0.5)(x)\n","outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n","\n","model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","history6 = model.fit(train_data_preprocessed_int,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"9glJ0lZYOcxR","executionInfo":{"status":"error","timestamp":1697088028679,"user_tz":-120,"elapsed":251294,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"ff63f40b-0e21-41f9-d4a6-1c30abf0aa27"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","248/782 [========>.....................] - ETA: 8:35 - loss: 0.5245 - accuracy: 0.7402"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-db560fb64ca6>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m history6 = model.fit(train_data_preprocessed_int,\n\u001b[0m\u001b[1;32m     11\u001b[0m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           validation_data = val_data_preprocessed_int)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["The model with embedding layer is learns a lot faster, but still not fast and accurately enough to beat simple feed-forward model. Let's try transformers."],"metadata":{"id":"ISqmBkqqVDNi"}},{"cell_type":"markdown","source":["### Build transformer encoder"],"metadata":{"id":"Ir7cdf5JQfrT"}},{"cell_type":"code","source":["class TransformerEncoder(tf.keras.layers.Layer):\n","  def __init__(self, num_heads, embed_dim, hidden_units, **kwargs):\n","    super().__init__(**kwargs)\n","    # Assign attributes\n","    self.num_heads = num_heads\n","    self.embed_dim = embed_dim\n","    self.hidden_units = hidden_units\n","\n","    # Define all model's layers\n","    self.mha = tf.keras.layers.MultiHeadAttention(num_heads = self.num_heads, key_dim = self.embed_dim)\n","    self.dense = tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(units = self.hidden_units, activation = 'relu'),\n","        tf.keras.layers.Dense(units = self.embed_dim)\n","    ])\n","    # Normalization layer normalizes each sequence independently, opposite to BatchNormalization\n","    self.layer_normalization1 = tf.keras.layers.LayerNormalization()\n","    self.layer_normalization2 = tf.keras.layers.LayerNormalization()\n","\n","  # Define forward propagation\n","  def call(self, inputs, mask = None):\n","    if mask is not None:\n","      # Since mask generated by embedding layer will be 2D we must expand its dims to be 3D (MHA expects 3D or 4D inputs)\n","      mask = mask[:, tf.newaxis, :]\n","    attention_output = self.mha(inputs, inputs, attention_mask = mask)\n","    proj_input = self.layer_normalization1(inputs + attention_output)\n","    proj_output = self.dense(proj_input)\n","    outputs = self.layer_normalization2(proj_input + proj_output)\n","    return outputs\n","\n","  # Define get_config method, to be able to save and load model using this layer\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","                  \"embed_dim\": self.embed_dim,\n","                  \"num_heads\": self.num_heads,\n","                  \"dense_dim\": self.hidden_units,\n","                  })\n","    return config\n"],"metadata":{"id":"o2YszYt2czDX","executionInfo":{"status":"ok","timestamp":1697091444602,"user_tz":-120,"elapsed":362,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Build PositionalEncoding layer to take into account word positions in sequence"],"metadata":{"id":"1vnnewHmjhBW"}},{"cell_type":"code","source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n",""],"metadata":{"id":"HI3X1XX2jpFd"},"execution_count":null,"outputs":[]}]}