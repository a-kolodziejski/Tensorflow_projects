{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQV3SrPRlm/4Im2c6COHBY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"l9rraAgkIuDt"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"xhqZMTfnDLkM","executionInfo":{"status":"ok","timestamp":1698305843981,"user_tz":-120,"elapsed":5991,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Load data (raw imdb dataset)"],"metadata":{"id":"nZM7lRb4I1OW"}},{"cell_type":"code","source":["!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vh0K2SJbJ71u","executionInfo":{"status":"ok","timestamp":1698305867566,"user_tz":-120,"elapsed":21510,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"5818ac84-f021-4c25-a912-a8efad6e025f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-26 07:37:25--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  23.6MB/s    in 4.8s    \n","\n","2023-10-26 07:37:31 (16.8 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"]}]},{"cell_type":"code","source":["# Remove train/unsup subdirectory\n","!rm -r aclImdb/train/unsup"],"metadata":{"id":"5E584s7qFGlT","executionInfo":{"status":"ok","timestamp":1698305872052,"user_tz":-120,"elapsed":2026,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Prepare validation data (50% of test data, so alltogether we have )\n","# Imports\n","import os, shutil, random\n","\n","BASE_DIR_TEST = \"aclImdb/test\"\n","ALL_DIR_POS = os.listdir(BASE_DIR_TEST + '/pos')\n","ALL_DIR_NEG = os.listdir(BASE_DIR_TEST + '/neg')\n","\n","# print(len(ALL_DIR_POS), len(ALL_DIR_NEG))\n","\n","# Define number of validation samples\n","VAL_SAMPLES = int(0.5 * len(ALL_DIR_POS + ALL_DIR_NEG))\n","\n","# Shuffle files\n","random.shuffle(ALL_DIR_POS)\n","random.shuffle(ALL_DIR_NEG)\n","\n","# Pick appropriate number of validation files\n","val_paths_pos = ALL_DIR_POS[:VAL_SAMPLES//2] # half samples will be positive\n","val_paths_neg = ALL_DIR_NEG[:VAL_SAMPLES//2] # half of samples will be negative\n","\n","# Create directories appropriate for validation files\n","os.makedirs('aclImdb/val/pos')\n","os.makedirs('aclImdb/val/neg')\n","\n","# Move all validation files into val/pos and val/neg directories\n","for file in val_paths_pos:\n","  shutil.move(src = \"aclImdb/test/pos/\" + file, dst = \"aclImdb/val/pos/\" + file)\n","\n","for file in val_paths_neg:\n","  shutil.move(src = \"aclImdb/test/neg/\" + file, dst = \"aclImdb/val/neg/\" + file)"],"metadata":{"id":"b8K6_vZnKgGk","executionInfo":{"status":"ok","timestamp":1698305876031,"user_tz":-120,"elapsed":668,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Create train, validation and test datasets using `text_dataset_from_directory`"],"metadata":{"id":"7ibTI4BA3-xP"}},{"cell_type":"code","source":["BATCH_SIZE = 32\n","\n","# Create train dataset\n","train_data = tf.keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size = BATCH_SIZE)\n","\n","# Create validation dataset\n","val_data = tf.keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size = BATCH_SIZE)\n","\n","# Create test dataset\n","test_data = tf.keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size = BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zWGTVz9x5SrQ","executionInfo":{"status":"ok","timestamp":1698305884070,"user_tz":-120,"elapsed":3506,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"83eb27de-fde9-4e4f-dada-e18c105795ec"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 2 classes.\n","Found 12500 files belonging to 2 classes.\n","Found 12500 files belonging to 2 classes.\n"]}]},{"cell_type":"markdown","source":["# First attempt: bag of words model"],"metadata":{"id":"k7Aw-r1l9PxD"}},{"cell_type":"markdown","source":["### Preprocess data with TextVectorization layer"],"metadata":{"id":"k10ApVRo9xGJ"}},{"cell_type":"code","source":["# Only consider single words (ngram = 1) and multi-hot encode each sentence\n","tv = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                       ngrams = 1,\n","                                       output_mode = 'multi_hot')\n","\n","# Extract all messages from training data (leave targets out)\n","train_data_text_only = train_data.map(lambda x, y: x)\n","\n","# Build vocabulary from training data using TextVectorizer\n","tv.adapt(train_data_text_only)\n","\n","# Prepare processed versions of datasets (text multi-hot encoded using vocabulary trained on training data)\n","train_data_preprocessed_1_gram = train_data.map(lambda x, y: (tv(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_1_gram = val_data.map(lambda x, y: (tv(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_1_gram = test_data.map(lambda x, y: (tv(x), y), num_parallel_calls = 4)\n"],"metadata":{"id":"NsxJA3Tm93OL","executionInfo":{"status":"ok","timestamp":1698305910851,"user_tz":-120,"elapsed":6222,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Check if everything OK\n","for inputs, targets in train_data_preprocessed_1_gram:\n","  print(\"inputs.shape:\", inputs.shape)\n","  print(\"inputs.dtype:\", inputs.dtype)\n","  print(\"targets.shape:\", targets.shape)\n","  print(\"targets.dtype:\", targets.dtype)\n","  print(\"inputs[0]:\", inputs[0])\n","  print(\"targets[0]:\", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rq0wDQs7D4pc","executionInfo":{"status":"ok","timestamp":1698305923093,"user_tz":-120,"elapsed":614,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"4840801f-1de2-4493-cadf-7f949f07589d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["### Build and train simple dense classification model\n","\n","def build_model(num_tokens = 20000, hidden_units = 16):\n","  inputs = tf.keras.layers.Input(shape = (num_tokens, ))\n","  x = tf.keras.layers.Dense(hidden_units, activation = 'relu')(inputs)\n","  x = tf.keras.layers.Dropout(rate = 0.5)(x)\n","  outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n","\n","  model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n","\n","  model.compile(optimizer = tf.keras.optimizers.Adam(),\n","              loss = 'binary_crossentropy',\n","              metrics = ['accuracy'])\n","\n","  return model\n","\n","model = build_model()\n","\n","history = model.fit(train_data_preprocessed_1_gram,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_1_gram)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tV2kmf7SEPZU","executionInfo":{"status":"ok","timestamp":1696933317488,"user_tz":-120,"elapsed":13197,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"1653d012-7ca3-4851-b8c8-10126110b74c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","782/782 [==============================] - 15s 18ms/step - loss: 0.4000 - accuracy: 0.8357 - val_loss: 0.2919 - val_accuracy: 0.8882\n","Epoch 2/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.2426 - accuracy: 0.9133 - val_loss: 0.2812 - val_accuracy: 0.8854\n","Epoch 3/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.1790 - accuracy: 0.9384 - val_loss: 0.2954 - val_accuracy: 0.8822\n","Epoch 4/10\n","782/782 [==============================] - 13s 17ms/step - loss: 0.1388 - accuracy: 0.9512 - val_loss: 0.3273 - val_accuracy: 0.8817\n","Epoch 5/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.1101 - accuracy: 0.9634 - val_loss: 0.3619 - val_accuracy: 0.8805\n","Epoch 6/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.0920 - accuracy: 0.9689 - val_loss: 0.3932 - val_accuracy: 0.8782\n","Epoch 7/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.0792 - accuracy: 0.9730 - val_loss: 0.4432 - val_accuracy: 0.8722\n","Epoch 8/10\n","782/782 [==============================] - 11s 14ms/step - loss: 0.0684 - accuracy: 0.9771 - val_loss: 0.4630 - val_accuracy: 0.8738\n","Epoch 9/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.0569 - accuracy: 0.9810 - val_loss: 0.5016 - val_accuracy: 0.8764\n","Epoch 10/10\n","782/782 [==============================] - 11s 15ms/step - loss: 0.0537 - accuracy: 0.9821 - val_loss: 0.5219 - val_accuracy: 0.8721\n"]}]},{"cell_type":"markdown","source":["#### We achieve ~ 89% accuracy. Let's try the same with 2-grams and 3-grams"],"metadata":{"id":"xGOLjqC7F7Xx"}},{"cell_type":"code","source":["# Build new TextVectorizers, one for 2-grams and one for 3-grams\n","tv2 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                     output_mode = 'multi_hot',\n","                                     ngrams = 2)\n","\n","tv3 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                     output_mode = 'multi_hot',\n","                                     ngrams = 3)\n","\n","# Train both both vectorizers on train data texsts\n","tv2.adapt(train_data_text_only)\n","tv3.adapt(train_data_text_only)\n","\n","# Prepare preprocessed versions of all datasets\n","train_data_preprocessed_2_gram = train_data.map(lambda x, y: (tv2(x), y), num_parallel_calls = 4)\n","train_data_preprocessed_3_gram = train_data.map(lambda x, y: (tv3(x), y), num_parallel_calls = 4)\n","\n","val_data_preprocessed_2_gram = val_data.map(lambda x, y: (tv2(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_3_gram = val_data.map(lambda x, y: (tv3(x), y), num_parallel_calls = 4)\n","\n","test_data_preprocessed_2_gram = test_data.map(lambda x, y: (tv2(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_3_gram = test_data.map(lambda x, y: (tv3(x), y), num_parallel_calls = 4)"],"metadata":{"id":"uFwdZ-XTHRMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build and train models\n","model2 = build_model()\n","\n","history2 = model.fit(train_data_preprocessed_2_gram,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_2_gram)\n","\n","model3 = build_model()\n","\n","history3 = model.fit(train_data_preprocessed_3_gram,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_3_gram)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idHj0BudIZjQ","executionInfo":{"status":"ok","timestamp":1696934206834,"user_tz":-120,"elapsed":360743,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"5989c952-69d9-468f-ce26-4153883a21ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","782/782 [==============================] - 17s 20ms/step - loss: 0.6591 - accuracy: 0.7088 - val_loss: 0.4234 - val_accuracy: 0.8382\n","Epoch 2/10\n","782/782 [==============================] - 18s 23ms/step - loss: 0.4367 - accuracy: 0.8155 - val_loss: 0.3786 - val_accuracy: 0.8653\n","Epoch 3/10\n","782/782 [==============================] - 14s 18ms/step - loss: 0.3612 - accuracy: 0.8576 - val_loss: 0.3487 - val_accuracy: 0.8718\n","Epoch 4/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.3045 - accuracy: 0.8832 - val_loss: 0.3555 - val_accuracy: 0.8742\n","Epoch 5/10\n","782/782 [==============================] - 18s 23ms/step - loss: 0.2667 - accuracy: 0.8988 - val_loss: 0.3691 - val_accuracy: 0.8774\n","Epoch 6/10\n","782/782 [==============================] - 15s 20ms/step - loss: 0.2310 - accuracy: 0.9161 - val_loss: 0.3748 - val_accuracy: 0.8785\n","Epoch 7/10\n","782/782 [==============================] - 14s 18ms/step - loss: 0.2085 - accuracy: 0.9254 - val_loss: 0.3900 - val_accuracy: 0.8813\n","Epoch 8/10\n","782/782 [==============================] - 14s 18ms/step - loss: 0.1909 - accuracy: 0.9318 - val_loss: 0.3990 - val_accuracy: 0.8818\n","Epoch 9/10\n","782/782 [==============================] - 18s 22ms/step - loss: 0.1728 - accuracy: 0.9406 - val_loss: 0.4301 - val_accuracy: 0.8855\n","Epoch 10/10\n","782/782 [==============================] - 17s 21ms/step - loss: 0.1595 - accuracy: 0.9446 - val_loss: 0.4462 - val_accuracy: 0.8884\n","Epoch 1/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.7102 - accuracy: 0.6544 - val_loss: 0.4936 - val_accuracy: 0.7951\n","Epoch 2/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.4848 - accuracy: 0.7829 - val_loss: 0.4346 - val_accuracy: 0.8252\n","Epoch 3/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.4054 - accuracy: 0.8327 - val_loss: 0.4098 - val_accuracy: 0.8472\n","Epoch 4/10\n","782/782 [==============================] - 17s 21ms/step - loss: 0.3597 - accuracy: 0.8585 - val_loss: 0.3932 - val_accuracy: 0.8611\n","Epoch 5/10\n","782/782 [==============================] - 15s 19ms/step - loss: 0.3230 - accuracy: 0.8797 - val_loss: 0.4124 - val_accuracy: 0.8648\n","Epoch 6/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.2909 - accuracy: 0.8950 - val_loss: 0.4277 - val_accuracy: 0.8680\n","Epoch 7/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.2679 - accuracy: 0.9059 - val_loss: 0.4501 - val_accuracy: 0.8710\n","Epoch 8/10\n","782/782 [==============================] - 16s 20ms/step - loss: 0.2434 - accuracy: 0.9157 - val_loss: 0.4592 - val_accuracy: 0.8715\n","Epoch 9/10\n","782/782 [==============================] - 18s 23ms/step - loss: 0.2175 - accuracy: 0.9253 - val_loss: 0.4801 - val_accuracy: 0.8742\n","Epoch 10/10\n","782/782 [==============================] - 17s 22ms/step - loss: 0.2104 - accuracy: 0.9284 - val_loss: 0.4977 - val_accuracy: 0.8741\n"]}]},{"cell_type":"markdown","source":["#### As we can see introduction of 2-grams and 3-grams did not help. It may be due to the fact, that for all experiments the same number of 20000 tokens was used"],"metadata":{"id":"F-PYk5bxI5qp"}},{"cell_type":"markdown","source":["### Using tf-idf output mode instead of multi-hot encoding"],"metadata":{"id":"YqbhFmq9MWjt"}},{"cell_type":"code","source":["# Create another TextVectorizer\n","tv4 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                        output_mode = 'tf-idf',\n","                                        ngrams = 1)\n","\n","# Adapt TV to training dataset\n","tv4.adapt(train_data_text_only)\n","\n","# Prepare preprocessed versions of all datasets\n","train_data_preprocessed_tfidf = train_data.map(lambda x, y: (tv4(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_tfidf = val_data.map(lambda x, y: (tv4(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_tfidf = test_data.map(lambda x, y: (tv4(x), y), num_parallel_calls = 4)"],"metadata":{"id":"hHHnjq5LM59N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if everything OK\n","for inputs, targets in train_data_preprocessed_tfidf:\n","  print(\"inputs.shape:\", inputs.shape)\n","  print(\"inputs.dtype:\", inputs.dtype)\n","  print(\"targets.shape:\", targets.shape)\n","  print(\"targets.dtype:\", targets.dtype)\n","  print(\"inputs[0]:\", inputs[0])\n","  print(\"targets[0]:\", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXtpYmKROIaZ","executionInfo":{"status":"ok","timestamp":1696935248886,"user_tz":-120,"elapsed":259,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"3c3a8b33-d5ed-4bd0-fd79-da37b83827cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([19.388176   6.2761817  1.4221123 ...  0.         0.         0.       ], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["# Build and train model\n","model4 = build_model()\n","\n","history4 = model.fit(train_data_preprocessed_tfidf,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_tfidf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZ7M9PceOAgo","executionInfo":{"status":"ok","timestamp":1696935427447,"user_tz":-120,"elapsed":157187,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"54d1f163-c24f-48d0-a229-1e47721d9633"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","782/782 [==============================] - 13s 17ms/step - loss: 0.4463 - accuracy: 0.8812 - val_loss: 0.7639 - val_accuracy: 0.8045\n","Epoch 2/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.2401 - accuracy: 0.9191 - val_loss: 0.8411 - val_accuracy: 0.8278\n","Epoch 3/10\n","782/782 [==============================] - 15s 19ms/step - loss: 0.1973 - accuracy: 0.9346 - val_loss: 0.8434 - val_accuracy: 0.8375\n","Epoch 4/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1618 - accuracy: 0.9409 - val_loss: 0.8106 - val_accuracy: 0.8422\n","Epoch 5/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1479 - accuracy: 0.9451 - val_loss: 0.8962 - val_accuracy: 0.8483\n","Epoch 6/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1377 - accuracy: 0.9501 - val_loss: 0.9066 - val_accuracy: 0.8500\n","Epoch 7/10\n","782/782 [==============================] - 12s 15ms/step - loss: 0.1217 - accuracy: 0.9550 - val_loss: 0.9450 - val_accuracy: 0.8521\n","Epoch 8/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1201 - accuracy: 0.9564 - val_loss: 0.8774 - val_accuracy: 0.8526\n","Epoch 9/10\n","782/782 [==============================] - 13s 16ms/step - loss: 0.1083 - accuracy: 0.9586 - val_loss: 0.8971 - val_accuracy: 0.8540\n","Epoch 10/10\n","782/782 [==============================] - 12s 16ms/step - loss: 0.1041 - accuracy: 0.9607 - val_loss: 0.9094 - val_accuracy: 0.8534\n"]}]},{"cell_type":"markdown","source":["#### We obtain ~85% accuracy"],"metadata":{"id":"qUOd2gpdOVXY"}},{"cell_type":"markdown","source":["# Second attempt: sequence models"],"metadata":{"id":"-iXbUDBhPDrK"}},{"cell_type":"markdown","source":["To implement sequence model, one has to do the following steps:\n","* represent samples as integer sequences (one integer for one word),\n","* map integer to a vector to obtain vector sequences,\n","* feed these vector sequences into 1-D CNN or RNN stack of layers"],"metadata":{"id":"D5LD7DGTPNg8"}},{"cell_type":"markdown","source":["### Define TextVectorizer which for each sentence outputs sequence of integers"],"metadata":{"id":"kiyOIQMoQXKx"}},{"cell_type":"code","source":["# Truncate each review to contain only 600 characters\n","tv5 = tf.keras.layers.TextVectorization(max_tokens = 20000,\n","                                        output_mode = 'int',\n","                                        output_sequence_length = 600)\n","\n","# Adapting TextVectorizer\n","tv5.adapt(train_data_text_only)\n","\n","# Transforming train, validation and test datasets\n","train_data_preprocessed_int = train_data.map(lambda x, y: (tv5(x), y), num_parallel_calls = 4)\n","val_data_preprocessed_int = val_data.map(lambda x, y: (tv5(x), y), num_parallel_calls = 4)\n","test_data_preprocessed_int = test_data.map(lambda x, y: (tv5(x), y), num_parallel_calls = 4)\n","\n","# Check if everything OK\n","for inputs, targets in train_data_preprocessed_int:\n","  print(\"inputs.shape:\", inputs.shape)\n","  print(\"inputs.dtype:\", inputs.dtype)\n","  print(\"targets.shape:\", targets.shape)\n","  print(\"targets.dtype:\", targets.dtype)\n","  print(\"inputs[0]:\", inputs[0])\n","  print(\"targets[0]:\", targets[0])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmICV_rKQhvb","executionInfo":{"status":"ok","timestamp":1698305974960,"user_tz":-120,"elapsed":6596,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"bfbc51fd-5232-44b0-d6ff-2a00369e5c2b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 600)\n","inputs.dtype: <dtype: 'int64'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor(\n","[ 4897  6348  1138     2     1     1     5     4   774   753  6348   141\n","     4  2683   483     2  4897     3    32     2  2411  7979    21     2\n","    81   506     5     2     1    13   252   108    11   204 13217    16\n","    30    86   164  2124    51     9    86   366    46    10   153   118\n","    87    50    11   204    59    63    28   302   148   101    10    67\n","   226   108    34   383    56   165  1328    14    12     2   204    14\n","    85    19    56  1328    14   234    36     2   872  4897  6348     7\n","    52    71    41     4   588   968   204     9     7    29     5     2\n","    88    74    91   218   206   611  1120   968   122  1115     3   134\n","    80     9  1141    21    34   473 17470    13    11   204  1116   234\n","  9468    12     5     2    18     9    14   448    21     3    10   103\n","     9     7     4    53    50   454    12   700    15     4  3434    17\n","     4  4432   839     7   491     6  1679   140    12   145   161    21\n","     2    62     1     5    20     2   278     5 10496  1062    33     4\n","   206   611   204   147     2   102     3    65  2354     1    17    62\n","     7  1324     3    41  1548     6     2  1215     6  4796  2780     3\n","    30    32     4   935     5  1275   485     3     4   169     5  8667\n","    33     2    32  1031     6     2  4012    13  4897  6348   794  3924\n","    85  2435   461     2   302   148    19    14   110  1561     5     2\n","  2868     5   748     2   114     3     9   498    97    16    47    53\n","   267    62  7347     5     2   120    13    13     2    86   164  2124\n","   370    58     6     2     1  2488     4   169    52    21     2  4559\n","     5  6749     3   267  1160  1833     2   649   977  5710     6   248\n","    81    45     9    66    22    16     2   203  8064 14977  3783    13\n","    13    36     2 11391     6     2 14091    48    14  3394    52   649\n","  3997    21   995     2 14977     3 11474  3023    21   721   101    11\n","  2124  4559     5     2  6749    14   210    61    34  1288    16  1464\n","  6348     8     4   269     5     1     1    13     2 11099   803     7\n","   235     2    88  1843    29     9    44     4  7317     5   649    12\n","  1610  1333   721  2253     8    60     2  4897     7   947    54  1041\n","    19     2   226   649   812    47    85     1   379    13    13     2\n"," 15804     3 16478  2106   295    80    35    26     2   167   155  2517\n","     3    57   706     1    35    24   191    50     1   255     2    86\n","   164   649     5     2 16478   803    24     4   111   808    80    35\n","   289     6    28    43   111    52    71  6348     3     1  2236   966\n","   101   966     6     2     1    13   663   101   803  1139  4559    14\n","  3454     6     2   967    60     8   106   763    14     4   896    80\n","     5     2   963     3   755   248     1  1315    21     2    81   545\n","     9    91    16    38   106    85   649     5     2  8300  4524    12\n","     2   635     5   862     5     2   120   124   926     3   265    87\n","    85     3  4157     2   858 13623    13    54  1234    10   256     2\n","   204    26     4   322   496   282    12  1745  1391     9     6   343\n","     2   531    10    26     7    12     9    96   138    21     2    83\n","   561    66    32    85  7658     3     2   204  1293    67   924     6\n","  1444     4   226   803  3493     6     2   445   862     5     2   204\n","    14    53     1     3    59    26    75    88  2409    19  1066   180\n","    24    15    35  7731    13     8     2   129    80     5     2   185\n","    12    10   492   279    30     4   111   264     6   160    12     9\n","   618     2   196   433   186     2    29  1401    33     2   161     5\n","  3062     5    83   126     2   259     5   302   148     7     4  1690], shape=(600,), dtype=int64)\n","targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["### Building RNN model"],"metadata":{"id":"LGMupKgCRZUo"}},{"cell_type":"code","source":["inputs = tf.keras.layers.Input(shape = (None, ), dtype = 'int64') # one input is sequence of integers\n","# Hot-encode each integer\n","encoded = tf.one_hot(inputs, depth = 20000)\n","x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(encoded)\n","x = tf.keras.layers.Dropout(0.5)(x)\n","outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","model = tf.keras.models.Model(inputs, outputs)\n","\n","model.compile(optimizer=\"adam\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","history5 = model.fit(train_data_preprocessed_int,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_int)"],"metadata":{"id":"KrXKGh_8RqFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### The above model is very slow, since each review is encoded as 600 x 20000 matrix, which has 12000000 elements. Better encoding would be necessary, such as word embedding"],"metadata":{"id":"flcBOML1TGBy"}},{"cell_type":"markdown","source":["### Same model as above but with embedding layer"],"metadata":{"id":"IECr2dK9T5HT"}},{"cell_type":"code","source":["inputs = tf.keras.layers.Input(shape = (600, ))\n","x = tf.keras.layers.Embedding(input_dim = 20000, output_dim = 256, mask_zero = True)(inputs) # Notice mask_zero parameter\n","x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(x)\n","x = tf.keras.layers.Dropout(rate = 0.5)(x)\n","outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n","\n","model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","history6 = model.fit(train_data_preprocessed_int,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_int)"],"metadata":{"id":"9glJ0lZYOcxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model with embedding layer is learns a lot faster, but still not fast and accurately enough to beat simple feed-forward model. Let's try transformers."],"metadata":{"id":"ISqmBkqqVDNi"}},{"cell_type":"markdown","source":["### Build transformer encoder"],"metadata":{"id":"Ir7cdf5JQfrT"}},{"cell_type":"code","source":["class TransformerEncoder(tf.keras.layers.Layer):\n","  def __init__(self, num_heads, embed_dim, hidden_units, **kwargs):\n","    super().__init__(**kwargs)\n","    # Assign attributes\n","    self.num_heads = num_heads\n","    self.embed_dim = embed_dim\n","    self.hidden_units = hidden_units\n","\n","    # Define all model's layers\n","    self.mha = tf.keras.layers.MultiHeadAttention(num_heads = self.num_heads, key_dim = self.embed_dim)\n","    self.dense = tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(units = self.hidden_units, activation = 'relu'),\n","        tf.keras.layers.Dense(units = self.embed_dim)\n","    ])\n","    # Normalization layer normalizes each sequence independently, opposite to BatchNormalization\n","    self.layer_normalization1 = tf.keras.layers.LayerNormalization()\n","    self.layer_normalization2 = tf.keras.layers.LayerNormalization()\n","\n","  # Define forward propagation\n","  def call(self, inputs, mask = None):\n","    if mask is not None:\n","      # Since mask generated by embedding layer will be 2D we must expand its dims to be 3D (MHA expects 3D or 4D inputs)\n","      mask = mask[:, tf.newaxis, :]\n","    attention_output = self.mha(inputs, inputs, attention_mask = mask)\n","    proj_input = self.layer_normalization1(inputs + attention_output)\n","    proj_output = self.dense(proj_input)\n","    outputs = self.layer_normalization2(proj_input + proj_output)\n","    return outputs\n","\n","  # Define get_config method, to be able to save and load model using this layer\n","  def get_config(self):\n","    config = super().get_config()\n","    config.update({\n","                  \"embed_dim\": self.embed_dim,\n","                  \"num_heads\": self.num_heads,\n","                  \"dense_dim\": self.hidden_units,\n","                  })\n","    return config\n"],"metadata":{"id":"o2YszYt2czDX","executionInfo":{"status":"ok","timestamp":1698306125922,"user_tz":-120,"elapsed":237,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Build PositionalEncoding layer to take into account word positions in sequence"],"metadata":{"id":"1vnnewHmjhBW"}},{"cell_type":"code","source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","    super().__init__(**kwargs)\n","    # Initialize attributes\n","    self.sequence_length = sequence_length\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","    # Define embedding layer for inputs (tokens)\n","    self.token_embeddings = tf.keras.layers.Embedding(input_dim = self.input_dim,\n","                                                      output_dim = self.output_dim)\n","    # Define embedding for positions of tokens in the sequence\n","    self.position_embeddings = tf.keras.layers.Embedding(input_dim = self.sequence_length,\n","                                                         output_dim = self.output_dim)\n","\n","  def call(self, inputs):\n","    # Get sequence length\n","    length = tf.shape(inputs)[-1]\n","    positions = tf.range(start=0, limit=length, delta=1)\n","    # Create embedding for tokens\n","    embedded_tokens = self.token_embeddings(inputs)\n","    # Create embedding for token positions\n","    embedded_positions = self.position_embeddings(positions)\n","    # Add and return both embeddings\n","    return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","      return tf.math.not_equal(inputs, 0)\n","\n","    # Define get_config method, to be able to save and load model using this layer\n","    def get_config(self):\n","      config = super().get_config()\n","      config.update({\n","                    \"output_dim\": self.output_dim,\n","                    \"sequence_length\": self.sequence_length,\n","                    \"input_dim\": self.input_dim,\n","                    })\n","      return config\n"],"metadata":{"id":"HI3X1XX2jpFd","executionInfo":{"status":"ok","timestamp":1698306160440,"user_tz":-120,"elapsed":5,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Train Transformer model with Positional Embedding layer"],"metadata":{"id":"8aY-LoVo5H5a"}},{"cell_type":"code","source":["inputs = tf.keras.layers.Input(shape = (600, ), dtype = 'int64')\n","x = PositionalEmbedding(input_dim = 20000, output_dim = 256, sequence_length = 600)(inputs)\n","x = TransformerEncoder(num_heads = 2, embed_dim = 256, hidden_units = 32)(x)\n","x = tf.keras.layers.GlobalMaxPooling1D()(x)\n","x = tf.keras.layers.Dropout(rate = 0.5)(x)\n","outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n","\n","model = tf.keras.models.Model(inputs = inputs, outputs = outputs)\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","history7 = model.fit(train_data_preprocessed_int,\n","          epochs = 10,\n","          validation_data = val_data_preprocessed_int)"],"metadata":{"id":"bZS-KgzS5Ro5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MX-L5lyh75m5"},"execution_count":null,"outputs":[]}]}