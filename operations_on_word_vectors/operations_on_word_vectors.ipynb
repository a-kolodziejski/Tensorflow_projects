{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3xjklmc3PxZJtg/dfphAY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Based on: `GloVe: Global Vectors for Word Representation` Jeffrey Pennington,   Richard Socher,   Christopher D. Manning, https://nlp.stanford.edu/projects/glove/"],"metadata":{"id":"qCV_C4FXa_ih"}},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"-MOmspaKcWco"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"F5NC-qkPcYe-","executionInfo":{"status":"ok","timestamp":1697693934643,"user_tz":-120,"elapsed":4,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Download pre-trained word vectors"],"metadata":{"id":"GpbIQSKySF0M"}},{"cell_type":"code","source":["!wget https://nlp.stanford.edu/data/glove.6B.zip\n","!unzip 'glove.6B.zip'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QqvPPNPas0E","executionInfo":{"status":"ok","timestamp":1697693712746,"user_tz":-120,"elapsed":188311,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"976808e3-ebc7-4fbb-d107-da751a9ae996"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-19 05:32:05--  https://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-10-19 05:32:05--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n","\n","2023-10-19 05:34:45 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}]},{"cell_type":"markdown","source":["There are 4 files in the archive and each file contains embedding vectors of different dimensionality (50, 100, 200, 300)."],"metadata":{"id":"KQrEsXe4clqU"}},{"cell_type":"markdown","source":["### Function to parse downloaded files"],"metadata":{"id":"WgoJyR1_d1dX"}},{"cell_type":"code","source":["def load_data(file_name):\n","  '''\n","  Loads data into list of vectors and disctionary that maps each word to its corresponding\n","  embedding vector\n","\n","  Args:\n","    file_name (string): name of the file to load\n","\n","  Returns:\n","    words (list): list of words in Glove embedding\n","    word_to_vec (dict): dictionary mapping word to embedding\n","  '''\n","  # Initialize list of words and mapping dictionary\n","  words = []\n","  word_to_vec = {}\n","\n","  with open(file_name, 'r') as file:\n","    for line in file:\n","      line_splitted = line.split()\n","      # First entry in each line is embedded word\n","      word = line_splitted[0]\n","      words.append(word)\n","      # The rest is vector representation of a word (map each entry to float)\n","      vector_representation = list(map(lambda x: float(x), line_splitted[1:]))\n","      word_to_vec[word] = np.array(vector_representation)\n","\n","  return words, word_to_vec\n"],"metadata":{"id":"HPHJHzoDeCsG","executionInfo":{"status":"ok","timestamp":1697695037707,"user_tz":-120,"elapsed":395,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Load 50-dimensional word representation"],"metadata":{"id":"OPvi3-kogFSj"}},{"cell_type":"code","source":["words, word_to_vec = load_data('glove.6B.50d.txt')"],"metadata":{"id":"nVAAYMjTgJ6H","executionInfo":{"status":"ok","timestamp":1697695050158,"user_tz":-120,"elapsed":9931,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Check if everything works\n","print(words[0]) #the\n","print(word_to_vec['the'])\n","print(type(word_to_vec['the']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gK6zlxSgVx2","executionInfo":{"status":"ok","timestamp":1697695085376,"user_tz":-120,"elapsed":4,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"d6565243-99d7-407d-ae43-1b3b1dd2c36a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["the\n","[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n"," -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n"," -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n"," -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n"," -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n","  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n","  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n"," -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n"," -1.1514e-01 -7.8581e-01]\n","<class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["The similarity between two words represented by vectors $v$ and $w$ is defined as the cosine of the angle between them. Hence\n","\n","$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u|| ||v||} = \\cos(\\theta) $"],"metadata":{"id":"SycpxMtic-0H"}},{"cell_type":"code","source":["def cosine_similarity(v, w):\n","  '''\n","  Computes cosine similarity measure between two vectors\n","\n","  Args:\n","    v (1-d array): vector representation of a word\n","    w (1-d array): vector representation of a word\n","\n","  Returns:\n","    distance (float): (cosine) distance between v and w\n","  '''\n","  # Calculate norms of both vectors\n","  norm_v = np.linalg.norm(v)\n","  norm_w = np.linalg.norm(w)\n","\n","  # Avoid division by 0\n","  if np.isclose(norm_v * norm_w, 0, atol=1e-32):\n","    return 0\n","\n","  # Calculate distance between v and w\n","  distance = np.dot(v, w)/(norm_v * norm_w)\n","  return distance"],"metadata":{"id":"1fQtI1cSa0Ve","executionInfo":{"status":"ok","timestamp":1697696339957,"user_tz":-120,"elapsed":678,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Find analogies between words"],"metadata":{"id":"ihPUmT02hSZ_"}},{"cell_type":"markdown","source":["The goal is to find $?$ such that $a$ is to $b$ as $c$ is to $?$. To do this we use vector representations of words. Since $a$ is related to $b$ and $c$ is related to $?$, then $v_b - v_a \\approx v_? - v_c$ which means that $\\text{CosineSimilarity(v_b - v_a, v_? - v_c)} $ is maximized."],"metadata":{"id":"yzAvTr4-hUsO"}},{"cell_type":"code","source":["def find_analogy(word_a, word_b, word_c, word_to_vec = word_to_vec):\n","  '''\n","  Finds analogy as described above: a is to b as c is to __\n","\n","  Args:\n","    word_a, word_b, word_c (strings): words composing an analogy\n","    word_to_vec (dict): dictionary mapping each vord to its corresponding vector\n","\n","  Returns:\n","    word_d (string): word that best matches the analogy\n","  '''\n","  # Get word embeddings for word_a, word_b, word_c\n","  vec_a = word_to_vec[word_a]\n","  vec_b = word_to_vec[word_b]\n","  vec_c = word_to_vec[word_c]\n","\n","  # Loop through all available words and find word_d such that\n","  # CosineSimilarity(vec_b - vec_a, vec_d - vec_c) is as high as possible\n","  word_d = None\n","  best_score = -100\n","  all_words = list(word_to_vec.keys())\n","\n","  for word in all_words:\n","    # to avoid best_word being one the input words, skip the input word_c\n","    if word == word_c:\n","      continue\n","    # Get embedding for word\n","    vec_word = word_to_vec[word]\n","    # Calculate similarity between v_b - v_a and v_word - v_c\n","    score = cosine_similarity(vec_b - vec_a, vec_word - vec_c)\n","    # Check if this gives best score so far\n","    if score > best_score:\n","      best_score = score\n","      word_d = word\n","\n","  return word_d"],"metadata":{"id":"EVmhAD1BiQAu","executionInfo":{"status":"ok","timestamp":1697696544308,"user_tz":-120,"elapsed":3,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Test find_analogy function\n","analogies = [('poland', 'polish', 'england'),\n","            ('man', 'king', 'woman'),\n","             ('man', 'woman', 'boy'),\n","             ('italy', 'rome', 'spain')]\n","\n","for elem in analogies:\n","  print(f\"{elem[0]} is to {elem[1]} as {elem[2]} is to {find_analogy(elem[0], elem[1], elem[2])}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQBZJjl0lmfL","executionInfo":{"status":"ok","timestamp":1697696871252,"user_tz":-120,"elapsed":77267,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"30cf54ac-7242-4e40-c60a-f23e44ac2d6b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["poland is to polish as england is to scottish\n","man is to king as woman is to king\n","man is to woman as boy is to girl\n","italy is to rome as spain is to rome\n"]}]},{"cell_type":"markdown","source":["As we can see, it works rather poorly. Let's try higher dimensional embeddings (300-dim instead of 50-dim)"],"metadata":{"id":"LXnSI9BpmUHd"}},{"cell_type":"code","source":["# Load 300-dim embeddings\n","words_300, word_to_vec_300 = load_data('glove.6B.300d.txt')"],"metadata":{"id":"FERBQRwbolCF","executionInfo":{"status":"ok","timestamp":1697697202617,"user_tz":-120,"elapsed":51436,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Comparison between 50-dim and 300-dim embeddings:\n","\n","analogies = [('poland', 'polish', 'england'),\n","            ('man', 'king', 'woman'),\n","             ('man', 'woman', 'boy'),\n","             ('italy', 'rome', 'spain')]\n","print(\"50-dim EMBEDDING FIND ANALOGY TASK:\")\n","for elem in analogies:\n","  print(f\"{elem[0]} is to {elem[1]} as {elem[2]} is to {find_analogy(elem[0], elem[1], elem[2])}\")\n","\n","print(\"300-dim EMBEDDING FIND ANALOGY TASK:\")\n","for elem in analogies:\n","  print(f\"{elem[0]} is to {elem[1]} as {elem[2]} is to {find_analogy(elem[0], elem[1], elem[2], word_to_vec = word_to_vec_300)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXVQ1UCzorYM","executionInfo":{"status":"ok","timestamp":1697697436892,"user_tz":-120,"elapsed":155001,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"b3161c20-a9e8-452d-8046-2aa2217306fb"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["50-dim EMBEDDING FIND ANALOGY TASK:\n","poland is to polish as england is to scottish\n","man is to king as woman is to king\n","man is to woman as boy is to girl\n","italy is to rome as spain is to rome\n","300-dim EMBEDDING FIND ANALOGY TASK:\n","poland is to polish as england is to english\n","man is to king as woman is to king\n","man is to woman as boy is to girl\n","italy is to rome as spain is to rome\n"]}]},{"cell_type":"markdown","source":["As we can see, 300-dim representation performs slightly better comapred to 50-dim embedding"],"metadata":{"id":"XT2w7Qw9pLTV"}},{"cell_type":"markdown","source":["## Remove gender bias from word embeddings"],"metadata":{"id":"6Deqf37Rrfd1"}},{"cell_type":"markdown","source":["To motivate the following, let's try to calculate cosine similarity between vector $g = v_{man} - v_{woman}$ which should (roughly) represent gender and couple of other word vectors"],"metadata":{"id":"eK8K9KeeriL9"}},{"cell_type":"code","source":["g = word_to_vec['man'] - word_to_vec['woman']\n","\n","words_to_check = ['computer', 'doctor', 'physics', 'science', 'nurse', 'singer', 'fashion', 'teacher']\n","\n","for word in words_to_check:\n","  print(word, cosine_similarity(word_to_vec[word], g))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XAuHyo8dsL4w","executionInfo":{"status":"ok","timestamp":1697698222117,"user_tz":-120,"elapsed":309,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"cd54c244-79dc-4cb3-fbe4-8ef5757981a3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["computer 0.10330358873850498\n","doctor -0.11895289410935041\n","physics 0.09697462160304735\n","science 0.06082906540929701\n","nurse -0.38030879680687524\n","singer -0.1850051813649629\n","fashion -0.03563894625772699\n","teacher -0.17920923431825664\n"]}]},{"cell_type":"markdown","source":["As we can see, some words tend to have gender stereotypes encoded in them. Let's try to remove gender bias from word vectors as proposed in paper `Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings` https://arxiv.org/abs/1607.06520. The idea is to project every word vector to a subspace orthogonal to $g$ (vector representing gender). That means we need to subtract from each vector its projection onto g:\n","\n","$v = v - \\frac{v \\cdot g}{||g||^2} g$"],"metadata":{"id":"ZT_4r3DTswyq"}},{"cell_type":"code","source":["# The paper assumes all word vectors to have L2 norm as 1\n","word_to_vec_unit_vectors = {\n","    word: embedding / np.linalg.norm(embedding)\n","    for word, embedding in word_to_vec.items()\n","}\n","g_unit = word_to_vec_unit_vectors['man'] - word_to_vec_unit_vectors['woman']"],"metadata":{"id":"fJdCi3fJvN3l","executionInfo":{"status":"ok","timestamp":1697698961097,"user_tz":-120,"elapsed":2686,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def remove_gender_bias(word, g = g_unit, word_to_vec = word_to_vec_unit_vectors):\n","  '''\n","  Removes gender bias from word as described above\n","\n","  Args:\n","    word (string): word to be debiased\n","    g (1-d array): vector representation of gender (v_{man} - v_{woman})\n","    word_to_vec (dict): dictionary mapping each vord to its corresponding vector\n","\n","  Returns:\n","    v_deb (1-d array): debiased word representation for word\n","  '''\n","  # Get vector representation of word\n","  vec_word = word_to_vec[word]\n","\n","  # Remove projection of v_vec onto g from v_vec\n","  v_deb = vec_word - (np.dot(vec_word, g)/np.linalg.norm(g)**2)*g\n","\n","  return v_deb"],"metadata":{"id":"f6Y-tZWRtUMo","executionInfo":{"status":"ok","timestamp":1697698966810,"user_tz":-120,"elapsed":348,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Test if everything works\n","word = 'nurse'\n","\n","print(f\"Cosine similarity before debiasing for '{word}' is {cosine_similarity(word_to_vec_unit_vectors[word], g_unit)}\")\n","\n","print(f\"Cosine similarity after debiasing for '{word}' is {cosine_similarity(remove_gender_bias(word), g_unit)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uybg7-w7vmja","executionInfo":{"status":"ok","timestamp":1697699154429,"user_tz":-120,"elapsed":261,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"7414df7f-3877-4e54-871f-f5426e385c9f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity before debiasing for 'nurse' is -0.3008480330254639\n","Cosine similarity after debiasing for 'nurse' is -4.6672835363612994e-17\n"]}]},{"cell_type":"markdown","source":["### Debiasing pairs of words"],"metadata":{"id":"iuWyK6RQwDnR"}},{"cell_type":"markdown","source":["To see another problem that can arise, let's try to (gender) debias words `actor`, `actress` and `babysit`"],"metadata":{"id":"KZg2s0p4ys4J"}},{"cell_type":"code","source":["vec_actor_debiased = remove_gender_bias('actor')\n","vec_actress_debiased = remove_gender_bias('actress')\n","vec_babysit_debiased = remove_gender_bias('babysit')"],"metadata":{"id":"Yox5_V1xy9qo","executionInfo":{"status":"ok","timestamp":1697700283855,"user_tz":-120,"elapsed":495,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["Now let's calculate cosine similarity between debiased words for `actor`, `actress` and `babysit`"],"metadata":{"id":"u44LX6cfzHa-"}},{"cell_type":"code","source":["print(f\"Cosine similarity between actor and babysit is {cosine_similarity(vec_actor_debiased, vec_babysit_debiased)}\")\n","print(f\"Cosine similarity between actress and babysit is {cosine_similarity(vec_actress_debiased, vec_babysit_debiased)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"814dRWlEzaEB","executionInfo":{"status":"ok","timestamp":1697700287036,"user_tz":-120,"elapsed":327,"user":{"displayName":"Adrian Kołodziejski","userId":"10042155241818787641"}},"outputId":"ad72ed98-21a4-40c0-bce5-05d8e9bcdc93"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between actor and babysit is 0.04888115571301206\n","Cosine similarity between actress and babysit is -0.012170527529280502\n"]}]},{"cell_type":"markdown","source":["As we can see, even though we debiased all words, `actor` and `actress` are not equidistant from `babysit`. The key idea behind equalization is to make sure that a particular pair of words are equidistant from the 49-dimensional $g_\\perp$. The equalization step also ensures that the two equalized steps are now the same distance from $e_{receptionist}^{debiased}$, or from any other word that has been neutralized. See Bolukbasi et al.\n","\n","* $\\mu = \\frac{e_{w1} + e_{w2}}{2}$\n","\n","* $ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n","$\n","\n","* $\\mu_{\\perp} = \\mu - \\mu_{B}$\n","\n","* $ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n","$\n","* $ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n","$\n","\n","\n","* $e_{w1B}^{corrected} = \\sqrt{{1 - ||\\mu_{\\perp} ||^2_2}} * \\frac{e_{\\text{w1B}} - \\mu_B} {||e_{w1B} - \\mu_B||_2}$\n","\n","\n","* $e_{w2B}^{corrected} = \\sqrt{{1 - ||\\mu_{\\perp} ||^2_2}} * \\frac{e_{\\text{w2B}} - \\mu_B} {||e_{w2B} - \\mu_B||_2}$\n","\n","* $e_1 = e_{w1B}^{corrected} + \\mu_{\\perp}$\n","* $e_2 = e_{w2B}^{corrected} + \\mu_{\\perp}$"],"metadata":{"id":"rZiozF4iz1NP"}},{"cell_type":"code","source":[],"metadata":{"id":"7tuTENsS1_Rc"},"execution_count":null,"outputs":[]}]}